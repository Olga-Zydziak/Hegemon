{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e1e0d52-7f05-4471-9f9d-8de9ded30e9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8333e9bcfa94406ba749a133514549a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Test', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd9aacb64e84fd3bdb801017ed5aa7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test czy callbacks w ogóle działają:\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "button = widgets.Button(description=\"Test\")\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_click(b):\n",
    "    with output:\n",
    "        print(\"Button clicked!\")\n",
    "\n",
    "button.on_click(on_click)\n",
    "display(button)\n",
    "display(output)\n",
    "\n",
    "# Kliknij button - powinno wypisać \"Button clicked!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79c03994-1129-4123-84fc-528f93f57f4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/hegemon_env/lib/python3.11/site-packages/pydantic/_internal/_model_construction.py:63: UserWarning: `validate_explainability_config` overrides an existing Pydantic `@model_validator` decorator\n",
      "  warnings.warn(f'`{k}` overrides an existing Pydantic `{existing.decorator_info.decorator_repr}` decorator')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from google.cloud import secretmanager\n",
    "\n",
    "#hegemon dependencis\n",
    "from hegemon.explainability.visualizer import HeatmapGenerator\n",
    "from hegemon.config.settings import get_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a082680-fa24-4f1d-83f6-e721ab2334b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit>=1.31.0 (from -r streamlit_app/requirements.txt (line 6))\n",
      "  Downloading streamlit-1.50.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting plotly>=5.18.0 (from -r streamlit_app/requirements.txt (line 9))\n",
      "  Downloading plotly-6.3.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting pandas>=2.1.0 (from -r streamlit_app/requirements.txt (line 10))\n",
      "  Using cached pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Collecting altair!=5.4.0,!=5.4.1,<6,>=4.0 (from streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6))\n",
      "  Using cached altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.5.0 (from streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6))\n",
      "  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: cachetools<7,>=4.0 in /opt/conda/envs/hegemon_env/lib/python3.11/site-packages (from streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6)) (6.2.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/conda/envs/hegemon_env/lib/python3.11/site-packages (from streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6)) (8.3.0)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /opt/conda/envs/hegemon_env/lib/python3.11/site-packages (from streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6)) (2.3.3)\n",
      "Requirement already satisfied: packaging<26,>=20 in /opt/conda/envs/hegemon_env/lib/python3.11/site-packages (from streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6)) (25.0)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /opt/conda/envs/hegemon_env/lib/python3.11/site-packages (from streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6)) (11.3.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in /opt/conda/envs/hegemon_env/lib/python3.11/site-packages (from streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6)) (5.29.5)\n",
      "Requirement already satisfied: pyarrow>=7.0 in /opt/conda/envs/hegemon_env/lib/python3.11/site-packages (from streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6)) (21.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /opt/conda/envs/hegemon_env/lib/python3.11/site-packages (from streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6)) (2.32.5)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /opt/conda/envs/hegemon_env/lib/python3.11/site-packages (from streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6)) (9.0.0)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6))\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /opt/conda/envs/hegemon_env/lib/python3.11/site-packages (from streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6)) (4.15.0)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6))\n",
      "  Using cached watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6))\n",
      "  Downloading gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6))\n",
      "  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /opt/conda/envs/hegemon_env/lib/python3.11/site-packages (from streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6)) (6.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/hegemon_env/lib/python3.11/site-packages (from pandas>=2.1.0->-r streamlit_app/requirements.txt (line 10)) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=2.1.0->-r streamlit_app/requirements.txt (line 10))\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=2.1.0->-r streamlit_app/requirements.txt (line 10))\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/hegemon_env/lib/python3.11/site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6)) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/envs/hegemon_env/lib/python3.11/site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6)) (4.25.1)\n",
      "Collecting narwhals>=1.14.2 (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6))\n",
      "  Downloading narwhals-2.8.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6))\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6))\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/envs/hegemon_env/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6)) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/hegemon_env/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/hegemon_env/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/hegemon_env/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6)) (2025.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/hegemon_env/lib/python3.11/site-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6)) (3.0.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/envs/hegemon_env/lib/python3.11/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6)) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/envs/hegemon_env/lib/python3.11/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6)) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/envs/hegemon_env/lib/python3.11/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6)) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/envs/hegemon_env/lib/python3.11/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.31.0->-r streamlit_app/requirements.txt (line 6)) (0.27.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/hegemon_env/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=2.1.0->-r streamlit_app/requirements.txt (line 10)) (1.17.0)\n",
      "Downloading streamlit-1.50.0-py3-none-any.whl (10.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n",
      "Using cached altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Downloading gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Using cached pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Using cached watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
      "Downloading plotly-6.3.1-py3-none-any.whl (9.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m117.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading narwhals-2.8.0-py3-none-any.whl (415 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, watchdog, tzdata, toml, smmap, narwhals, blinker, pydeck, plotly, pandas, gitdb, gitpython, altair, streamlit\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/14\u001b[0m [streamlit]14\u001b[0m [streamlit]\n",
      "\u001b[1A\u001b[2KSuccessfully installed altair-5.5.0 blinker-1.9.0 gitdb-4.0.12 gitpython-3.1.45 narwhals-2.8.0 pandas-2.3.3 plotly-6.3.1 pydeck-0.9.1 pytz-2025.2 smmap-5.0.2 streamlit-1.50.0 toml-0.10.2 tzdata-2025.2 watchdog-6.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r streamlit_app/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30e9cc50-90ae-45d3-a075-20cf7b631a6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📍 GCP Project: dark-data-discovery\n",
      "📍 Location: us-central1\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"dark-data-discovery\"\n",
    "LOCATION = \"us-central1\"\n",
    "\n",
    "print(f\"📍 GCP Project: {PROJECT_ID}\")\n",
    "print(f\"📍 Location: {LOCATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "145efff6-e199-47c6-a6e9-7a3abd60b915",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ANTHROPIC_API_KEY: **********uwAA\n"
     ]
    }
   ],
   "source": [
    "def get_secret(project_id, secret_id, version_id=\"latest\"):\n",
    "    client = secretmanager.SecretManagerServiceClient()\n",
    "    name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n",
    "    response = client.access_secret_version(request={\"name\": name})\n",
    "    return response.payload.data.decode(\"UTF-8\")\n",
    "\n",
    "try:\n",
    "    key = get_secret(PROJECT_ID, \"ANTHROPIC_API_KEY\")\n",
    "    print(f\"✅ ANTHROPIC_API_KEY: {'*' * 10}{key[-4:]}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to get ANTHROPIC_API_KEY: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "793044ce-63c8-492a-8605-b14460f89684",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment configured for Vertex AI\n",
      "   Explainability enabled: true\n",
      "   Classifier model: gemini-2.5-flash\n"
     ]
    }
   ],
   "source": [
    "# Komórka 1: Setup\n",
    "# ENABLE EXPLAINABILITY (przez os.environ)\n",
    "os.environ[\"HEGEMON_EXPLAINABILITY_ENABLED\"] = \"true\"\n",
    "os.environ[\"HEGEMON_EXPLAINABILITY_SEMANTIC_FINGERPRINT\"] = \"true\"\n",
    "os.environ[\"HEGEMON_EXPLAINABILITY_CLASSIFIER_MODEL\"] = \"gemini-2.5-flash\"  # Vertex AI model\n",
    "\n",
    "# Setup logging dla Jupyter\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    force=True\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✅ Environment configured for Vertex AI\")\n",
    "print(f\"   Explainability enabled: {os.environ.get('HEGEMON_EXPLAINABILITY_ENABLED')}\")\n",
    "print(f\"   Classifier model: {os.environ.get('HEGEMON_EXPLAINABILITY_CLASSIFIER_MODEL')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c9247d3-9fa2-4bbc-b956-60bd87240383",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 14:01:30,365 - hegemon.config.settings - WARNING - Explainability enabled but no Google API key found. Classifier will fail.\n",
      "2025-10-14 14:01:30,366 - hegemon.config.settings - INFO - ✅ Sceptyk will use Vertex AI (Application Default Credentials)\n",
      "2025-10-14 14:01:30,367 - hegemon.config.settings - INFO - ✅ All required authentication validated\n",
      "2025-10-14 14:01:30,369 - hegemon.config.settings - INFO - ✅ HEGEMON Settings initialized (GCP Project: dark-data-discovery)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explainability Layers:\n",
      "  Layer 6 (Semantic): True\n",
      "  Layer 2 (Epistemic): True\n",
      "Configuration:\n",
      "  Katalizator: claude-sonnet-4-5-20250929\n",
      "  Sceptyk: gemini-2.5-pro\n",
      "  Gubernator: claude-sonnet-4-5-20250929\n",
      "  Syntezator: claude-opus-4-1-20250805\n",
      "\n",
      "Debate settings:\n",
      "  Consensus threshold: 0.7\n",
      "  Max cycles: 5\n",
      "\n",
      "✅ SUCCESS: Explainability is ENABLED!\n",
      "   Proceed to next cells.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Komórka 2: Import Hegemon modules\n",
    "from hegemon.explainability.visualizer import HeatmapGenerator\n",
    "\n",
    "\n",
    "# Verify settings\n",
    "settings = get_settings()\n",
    "print(\"Explainability Layers:\")\n",
    "print(f\"  Layer 6 (Semantic): {settings.explainability_semantic_fingerprint}\")\n",
    "print(f\"  Layer 2 (Epistemic): {settings.explainability_epistemic_uncertainty}\")\n",
    "\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Katalizator: {settings.katalizator.model}\")\n",
    "print(f\"  Sceptyk: {settings.sceptyk.model}\")\n",
    "print(f\"  Gubernator: {settings.gubernator.model}\")\n",
    "print(f\"  Syntezator: {settings.syntezator.model}\")\n",
    "print(f\"\\nDebate settings:\")\n",
    "print(f\"  Consensus threshold: {settings.debate.consensus_threshold}\")\n",
    "print(f\"  Max cycles: {settings.debate.max_cycles}\")\n",
    "# CRITICAL CHECK\n",
    "if not settings.explainability_enabled:\n",
    "    print(\"\\n❌ ERROR: Explainability is DISABLED!\")\n",
    "    print(\"   This means environment variable was set AFTER settings were cached.\")\n",
    "    print(\"   SOLUTION: Restart kernel and run cells in correct order.\")\n",
    "else:\n",
    "    print(\"\\n✅ SUCCESS: Explainability is ENABLED!\")\n",
    "    print(\"   Proceed to next cells.\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88b576cb-b965-4e8c-addb-234b7d25fa96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Mission defined:\n",
      "\n",
      "Design a comprehensive AI strategy for a mid-size e-commerce company.\n",
      "\n",
      "Context:\n",
      "- Company: 150 employees, $50M annual revenue\n",
      "- Current tech stack: Shopify + basic analytics\n",
      "- Budget: $500K annually for AI initiatives\n",
      "- Timeline: 12 months\n",
      "\n",
      "Requirements:\n",
      "1. Improve customer experience (personalization, recommendations)\n",
      "2. Reduce operational costs (automation, optimization)\n",
      "3. GDPR compliant\n",
      "4. Limited ML expertise in-house\n",
      "\n",
      "Deliverables needed:\n",
      "- Technology stack recommendations\n",
      "- Phased implementation roadmap (Q1-Q4)\n",
      "- Team structure and hiring plan\n",
      "- Risk analysis\n",
      "- Success metrics (KPIs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zdefiniuj swoją misję\n",
    "MISSION = \"\"\"\n",
    "Design a comprehensive AI strategy for a mid-size e-commerce company.\n",
    "\n",
    "Context:\n",
    "- Company: 150 employees, $50M annual revenue\n",
    "- Current tech stack: Shopify + basic analytics\n",
    "- Budget: $500K annually for AI initiatives\n",
    "- Timeline: 12 months\n",
    "\n",
    "Requirements:\n",
    "1. Improve customer experience (personalization, recommendations)\n",
    "2. Reduce operational costs (automation, optimization)\n",
    "3. GDPR compliant\n",
    "4. Limited ML expertise in-house\n",
    "\n",
    "Deliverables needed:\n",
    "- Technology stack recommendations\n",
    "- Phased implementation roadmap (Q1-Q4)\n",
    "- Team structure and hiring plan\n",
    "- Risk analysis\n",
    "- Success metrics (KPIs)\n",
    "\"\"\"\n",
    "\n",
    "print(\"📋 Mission defined:\")\n",
    "print(MISSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e63d28df-10ea-433f-9097-0a6489b4af12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Selected mode: reviewer\n",
      "\n",
      "Expected checkpoints:\n",
      "  - POST_THESIS (po każdym Katalizatorze)\n",
      "  - POST_EVALUATION (po Gubernatorze)\n",
      "  - PRE_SYNTHESIS (przed Syntetatorem)\n"
     ]
    }
   ],
   "source": [
    "# Wybierz tryb\n",
    "MODE = \"reviewer\"  # Zmień na \"observer\" lub \"collaborator\"\n",
    "\n",
    "print(f\"✅ Selected mode: {MODE}\")\n",
    "print(f\"\\nExpected checkpoints:\")\n",
    "if MODE == \"observer\":\n",
    "    print(\"  - PRE_SYNTHESIS (1x przed syntezą)\")\n",
    "elif MODE == \"reviewer\":\n",
    "    print(\"  - POST_THESIS (po każdym Katalizatorze)\")\n",
    "    print(\"  - POST_EVALUATION (po Gubernatorze)\")\n",
    "    print(\"  - PRE_SYNTHESIS (przed Syntetatorem)\")\n",
    "else:\n",
    "    print(\"  - All checkpoints + detailed guidance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a8b5562-e5d7-47ad-957d-d637aabdc69d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Główne importy HITL\n",
    "from hegemon.graph_hitl_v3 import (\n",
    "    create_hegemon_graph_hitl_v3,\n",
    "    run_debate_hitl_v3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "500019b7-1851-4f26-9d6c-9550c74e5c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); \n",
       "                    color: white; padding: 20px; border-radius: 10px; margin-bottom: 20px;'>\n",
       "            <h2 style='margin: 0;'>🔍 Checkpoint: Post Thesis</h2>\n",
       "            <p style='margin: 10px 0 0 0;'>\n",
       "                <strong>Agent:</strong> Katalizator | \n",
       "                <strong>Cycle:</strong> 1 | \n",
       "                <strong>Confidence:</strong> 0.55\n",
       "            </p>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style='background: #f8f9fa; padding: 15px; border-left: 4px solid #667eea; margin-bottom: 20px;'>\n",
       "            <h3 style='margin-top: 0;'>📋 Executive Summary</h3>\n",
       "            <p>Katalizator completed analysis in cycle 1. Review the output below for key claims and concerns.</p>\n",
       "            <h4>Key Points:</h4>\n",
       "            <ul>\n",
       "                <li>Katalizator completed analysis in cycle 1</li><li>Review the output below for key claims and concerns</li>\n",
       "            </ul>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>⚠️ Important Claims</h3>\n",
       "            <div style='border-left: 4px solid #4CAF50; padding: 10px; margin: 10px 0; background: #f9f9f9;'>\n",
       "                <strong>Confidence: 0.80</strong> - high_impact<br>\n",
       "                <p style='margin: 5px 0;'>The critical realization from the first cycle is that neither velocity-first nor foundation-first approaches alone address the company&#x27;s core constraint: translating a $500K budget into measurable revenue impact without ML expertise. The winning strategy is a &quot;proven ROI ladder&quot; where each quarter&#x27;s...</p>\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style='border: 1px solid #ddd; padding: 15px; margin: 20px 0; border-radius: 5px;'>\n",
       "            <h3>📄 Full Output</h3>\n",
       "            <pre style='white-space: pre-wrap; background: #f5f5f5; padding: 10px; border-radius: 5px;'>The critical realization from the first cycle is that neither velocity-first nor foundation-first approaches alone address the company&#x27;s core constraint: translating a $500K budget into measurable revenue impact without ML expertise. The winning strategy is a &quot;proven ROI ladder&quot; where each quarter&#x27;s AI investment must demonstrably fund the next quarter&#x27;s initiatives through cost savings or revenue gains, creating a self-sustaining implementation cycle.\n",
       "\n",
       "The foundation begins in Q1 with two parallel tracks requiring $120K investment. First, implement Clerk.io or Nosto for product recommendations, targeting a conservative 12% basket size increase on the company&#x27;s $50M revenue base—this alone generates $500K in incremental annual revenue at typical 8% margins, funding the entire year&#x27;s AI budget. Second, deploy Zendesk AI or Intercom&#x27;s Resolution Bot to automate 40% of tier-1 support tickets, saving approximately $80K annually in customer service costs while maintaining response quality. These are battle-tested solutions with documented ROI in comparable e-commerce environments.\n",
       "\n",
       "Q2 focuses on retention economics with $150K allocated to Klaviyo&#x27;s predictive analytics and Yotpo&#x27;s review intelligence platform. The goal is reducing customer acquisition cost dependency by improving repeat purchase rates from industry average 27% to 35%, which at $50M revenue scale represents $2.4M in retained customer lifetime value. Simultaneously, establish a lightweight AI governance framework and GDPR compliance protocols, budgeting $80K for legal review and documentation.\n",
       "\n",
       "Q3-Q4 deploys the accumulated savings and revenue gains into inventory optimization using tools like Inventory Planner, targeting 15% reduction in carrying costs and 20% decrease in stockouts. The final phase includes hiring one AI product manager ($120K annually) whose mandate is institutionalizing these capabilities and identifying the next ROI ladder opportunities.\n",
       "\n",
       "Success metrics center on capital efficiency: each dollar invested must generate $3 in measurable value within six months. This approach transforms AI from a cost center requiring faith into a profit engine with quarterly proof points, making it defensible to stakeholders while building organizational AI literacy through tangible wins.</pre>\n",
       "            \n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2932b3be65864c4381515036ac0b4584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>👤 Your Feedback</h3>'), RadioButtons(description='Decision:', options=(('✅ Appr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<p style='color: orange;'>⚠️ Timeout - Auto-approving...</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 14:31:32,177 - hegemon.agents - INFO - ⚔️ SCEPTYK (google/gemini-2.5-pro via Vertex AI): Generating Antithesis (Cycle 1)\n",
      "2025-10-14 14:31:32,179 - langchain_google_vertexai.chat_models - WARNING - Unexpected argument 'timeout' provided to ChatVertexAI.\n",
      "/home/jupyter/olga_zydziak/version_beta/Folder/hegemon_mvp/hegemon/agents.py:301: UserWarning: WARNING! timeout is not default parameter.\n",
      "                timeout was transferred to model_kwargs.\n",
      "                Please confirm that timeout is what you intended.\n",
      "  llm = get_llm_for_agent(\"Sceptyk\")\n",
      "2025-10-14 14:31:58,023 - hegemon.agents - INFO - ✅ SCEPTYK: Antithesis generated (1935 chars)\n",
      "2025-10-14 14:31:58,024 - hegemon.explainability.collector - INFO - 🔍 Collecting explainability for Sceptyk (Cycle 1)\n",
      "2025-10-14 14:32:07,507 - hegemon.explainability.classifier - INFO - ✅ Classification successful: 9482ms, top_concept=('caution', 0.8)\n",
      "2025-10-14 14:32:13,285 - hegemon.explainability.epistemic - INFO - Claim extraction successful: 13 claims, 5777ms, avg_conf=0.75\n",
      "2025-10-14 14:32:13,286 - hegemon.explainability.collector - INFO - ✅ Explainability collected for Sceptyk: L6(9482ms), L2(5777ms, 13 claims)\n",
      "2025-10-14 14:32:13,288 - hegemon.agents - INFO - ⚖️ GUBERNATOR (anthropic/claude-sonnet-4-5-20250929): Evaluating Consensus (Cycle 1)\n",
      "2025-10-14 14:32:22,869 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "2025-10-14 14:32:22,871 - anthropic._base_client - INFO - Retrying request to /v1/messages in 0.427624 seconds\n",
      "2025-10-14 14:33:23,361 - anthropic._base_client - INFO - Retrying request to /v1/messages in 0.849809 seconds\n",
      "2025-10-14 14:33:28,454 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n"
     ]
    },
    {
     "ename": "OverloadedError",
     "evalue": "Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}, 'request_id': None}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverloadedError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Run debate (INTERACTIVE - będzie czekać na Twój input!)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m final_state \u001b[38;5;241m=\u001b[39m \u001b[43mrun_debate_hitl_v3\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmission\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMISSION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintervention_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Debate Complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/olga_zydziak/version_beta/Folder/hegemon_mvp/hegemon/graph_hitl_v3.py:270\u001b[0m, in \u001b[0;36mrun_debate_hitl_v3\u001b[0;34m(mission, intervention_mode)\u001b[0m\n\u001b[1;32m    259\u001b[0m graph \u001b[38;5;241m=\u001b[39m create_hegemon_graph_hitl_v3()\n\u001b[1;32m    261\u001b[0m initial_state \u001b[38;5;241m=\u001b[39m DebateStateHITL(\n\u001b[1;32m    262\u001b[0m     mission\u001b[38;5;241m=\u001b[39mmission,\n\u001b[1;32m    263\u001b[0m     contributions\u001b[38;5;241m=\u001b[39m[],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m     hitl_enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    268\u001b[0m )\n\u001b[0;32m--> 270\u001b[0m final_state \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_state\n",
      "File \u001b[0;32m/opt/conda/envs/hegemon_env/lib/python3.11/site-packages/langgraph/pregel/main.py:3068\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[1;32m   3065\u001b[0m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m Any] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3066\u001b[0m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 3068\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3069\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupdates\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m   3073\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m   3074\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3076\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3077\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3078\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3080\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3081\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3082\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m   3083\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/hegemon_env/lib/python3.11/site-packages/langgraph/pregel/main.py:2657\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2655\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[1;32m   2656\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2657\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2658\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2662\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2663\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[1;32m   2664\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2665\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmpty\u001b[49m\n\u001b[1;32m   2666\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2667\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
      "File \u001b[0;32m/opt/conda/envs/hegemon_env/lib/python3.11/site-packages/langgraph/pregel/_runner.py:162\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    160\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/conda/envs/hegemon_env/lib/python3.11/site-packages/langgraph/pregel/_retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m/opt/conda/envs/hegemon_env/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:657\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m--> 657\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/opt/conda/envs/hegemon_env/lib/python3.11/site-packages/langgraph/_internal/_runnable.py:401\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    399\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/olga_zydziak/version_beta/Folder/hegemon_mvp/hegemon/agents.py:376\u001b[0m, in \u001b[0;36mgubernator_node\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    369\u001b[0m llm_with_structure \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mwith_structured_output(GovernorEvaluation)\n\u001b[1;32m    371\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    372\u001b[0m     SystemMessage(content\u001b[38;5;241m=\u001b[39msystem_prompt),\n\u001b[1;32m    373\u001b[0m     HumanMessage(content\u001b[38;5;241m=\u001b[39muser_prompt),\n\u001b[1;32m    374\u001b[0m ]\n\u001b[0;32m--> 376\u001b[0m evaluation: GovernorEvaluation \u001b[38;5;241m=\u001b[39m \u001b[43mllm_with_structure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;66;03m# IMPORTANT: Define ALL variables BEFORE explainability collection\u001b[39;00m\n\u001b[1;32m    379\u001b[0m evaluation_content \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluation\u001b[38;5;241m.\u001b[39mevaluation_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsensus Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluation\u001b[38;5;241m.\u001b[39mconsensus_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRationale: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluation\u001b[38;5;241m.\u001b[39mrationale\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/hegemon_env/lib/python3.11/site-packages/langchain_core/runnables/base.py:3243\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[1;32m   3242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 3243\u001b[0m         input_ \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3244\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3245\u001b[0m         input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config)\n",
      "File \u001b[0;32m/opt/conda/envs/hegemon_env/lib/python3.11/site-packages/langchain_core/runnables/base.py:5710\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5703\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   5704\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5705\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5708\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5709\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5711\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5712\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5713\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5714\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/hegemon_env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:395\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    391\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    392\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m--> 395\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    405\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/opt/conda/envs/hegemon_env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1023\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1021\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m   1022\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m-> 1023\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/hegemon_env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:840\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    839\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 840\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    846\u001b[0m         )\n\u001b[1;32m    847\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    848\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/conda/envs/hegemon_env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1089\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1087\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1089\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1090\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1093\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/hegemon_env/lib/python3.11/site-packages/langchain_anthropic/chat_models.py:1760\u001b[0m, in \u001b[0;36mChatAnthropic._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1758\u001b[0m payload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_request_payload(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1760\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m anthropic\u001b[38;5;241m.\u001b[39mBadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1762\u001b[0m     _handle_anthropic_bad_request(e)\n",
      "File \u001b[0;32m/opt/conda/envs/hegemon_env/lib/python3.11/site-packages/langchain_anthropic/chat_models.py:1619\u001b[0m, in \u001b[0;36mChatAnthropic._create\u001b[0;34m(self, payload)\u001b[0m\n\u001b[1;32m   1617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m payload:\n\u001b[1;32m   1618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mbeta\u001b[38;5;241m.\u001b[39mmessages\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpayload)\n\u001b[0;32m-> 1619\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/hegemon_env/lib/python3.11/site-packages/anthropic/_utils/_utils.py:282\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/hegemon_env/lib/python3.11/site-packages/anthropic/resources/messages/messages.py:930\u001b[0m, in \u001b[0;36mMessages.create\u001b[0;34m(self, max_tokens, messages, model, metadata, service_tier, stop_sequences, stream, system, temperature, thinking, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m DEPRECATED_MODELS:\n\u001b[1;32m    924\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    925\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe model \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will reach end-of-life on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEPRECATED_MODELS[model]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPlease migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m    927\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m    928\u001b[0m     )\n\u001b[0;32m--> 930\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/v1/messages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop_sequences\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthinking\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mthinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessageCreateParamsStreaming\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessageCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRawMessageStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/hegemon_env/lib/python3.11/site-packages/anthropic/_base_client.py:1324\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1311\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1312\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1321\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1322\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1323\u001b[0m     )\n\u001b[0;32m-> 1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/envs/hegemon_env/lib/python3.11/site-packages/anthropic/_base_client.py:1112\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1109\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1111\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1112\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mOverloadedError\u001b[0m: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}, 'request_id': None}",
      "\u001b[0mDuring task with name 'gubernator' and id '92f37ae2-be4e-0cd8-34a2-ab131ce75224'"
     ]
    }
   ],
   "source": [
    "# 🚀 START DEBATE!\n",
    "print(\"=\"*80)\n",
    "print(\"🤖 Starting HITL Debate...\")\n",
    "print(\"=\"*80)\n",
    "# Run debate (INTERACTIVE - będzie czekać na Twój input!)\n",
    "final_state = run_debate_hitl_v3(\n",
    "    mission=MISSION,\n",
    "    intervention_mode=MODE,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ Debate Complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50084d51-c98e-4d2b-93f2-fa4870f047b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"📊 DEBATE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total cycles: {final_state.cycle_count}\")\n",
    "print(f\"Total contributions: {len(final_state.contributions)}\")\n",
    "print(f\"Human feedbacks provided: {len(final_state.human_feedback)}\")\n",
    "print(f\"Revisions requested: {final_state.revision_count}\")\n",
    "print(f\"Final consensus score: {final_state.current_consensus_score:.2f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212672b9-6511-4aeb-9bfc-10667b4c08ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedback breakdown\n",
    "if final_state.human_feedback:\n",
    "    print(\"📝 YOUR FEEDBACK HISTORY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, feedback in enumerate(final_state.human_feedback, 1):\n",
    "        print(f\"\\n{i}. Checkpoint: {feedback.checkpoint.value}\")\n",
    "        print(f\"   Decision: {feedback.decision.value}\")\n",
    "        print(f\"   Timestamp: {feedback.timestamp.strftime('%H:%M:%S')}\")\n",
    "        \n",
    "        if feedback.guidance:\n",
    "            print(f\"   Your guidance: {feedback.guidance[:100]}...\")\n",
    "        \n",
    "        if feedback.priority_claims:\n",
    "            print(f\"   Priority claims: {len(feedback.priority_claims)}\")\n",
    "        \n",
    "        if feedback.flagged_concerns:\n",
    "            print(f\"   Flagged concerns: {len(feedback.flagged_concerns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b776096-d8e8-416e-b003-d66bd3d4f228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Plan overview\n",
    "if final_state.final_plan:\n",
    "    print(\"\\n📋 FINAL PLAN OVERVIEW\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    plan = final_state.final_plan\n",
    "    \n",
    "    print(f\"\\nMission: {plan.mission_overview[:200]}...\")\n",
    "    print(f\"\\nRequired Agents: {len(plan.required_agents)}\")\n",
    "    for agent in plan.required_agents[:5]:  # Show first 5\n",
    "        print(f\"  - {agent.role}: {', '.join(agent.skills[:3])}\")\n",
    "    \n",
    "    print(f\"\\nWorkflow Steps: {len(plan.workflow)}\")\n",
    "    for step in plan.workflow[:5]:  # Show first 5\n",
    "        print(f\"  {step.step_number}. {step.description[:60]}...\")\n",
    "        print(f\"     Agent: {step.assigned_agent} | Duration: {step.estimated_duration}\")\n",
    "    \n",
    "    print(f\"\\nTotal workflow duration: {plan.total_estimated_duration}\")\n",
    "    print(f\"Risk score: {plan.overall_risk_score:.2f}\")\n",
    "else:\n",
    "    print(\"\\n⚠️ No final plan generated (debate may have been rejected)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128a1bc7-ac60-44a9-960d-cd6da58eb0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to JSON\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "output_dir = Path(\"output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = output_dir / f\"hegemon_hitl_{MODE}_{timestamp}.json\"\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(\n",
    "        final_state.model_dump(mode='json'),\n",
    "        f,\n",
    "        indent=2,\n",
    "        ensure_ascii=False,\n",
    "    )\n",
    "\n",
    "print(f\"💾 Results saved to: {output_file}\")\n",
    "print(f\"   File size: {output_file.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf68881-2339-415c-b264-37f378e9eb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all agent contributions\n",
    "print(\"🔍 FULL DEBATE HISTORY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for contrib in final_state.contributions:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Agent: {contrib.agent_id} | Type: {contrib.type} | Cycle: {contrib.cycle}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\n{contrib.content[:500]}...\")\n",
    "    print(f\"\\nRationale: {contrib.rationale[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d751311-8b0a-4088-9507-cb7f41feea64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "from collections import Counter\n",
    "\n",
    "print(\"📈 PERFORMANCE METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Intervention metrics\n",
    "intervention_rate = len(final_state.human_feedback) / max(final_state.cycle_count, 1)\n",
    "print(f\"\\nIntervention rate: {intervention_rate:.2f} feedbacks/cycle\")\n",
    "\n",
    "# Decision breakdown\n",
    "if final_state.human_feedback:\n",
    "    decisions = Counter(f.decision.value for f in final_state.human_feedback)\n",
    "    total = len(final_state.human_feedback)\n",
    "    \n",
    "    print(f\"\\nDecision breakdown:\")\n",
    "    for decision, count in decisions.items():\n",
    "        percentage = (count / total) * 100\n",
    "        print(f\"  {decision}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    approval_rate = decisions.get('approve', 0) / total\n",
    "    print(f\"\\nApproval rate: {approval_rate:.1%}\")\n",
    "\n",
    "# Revision metrics\n",
    "revision_rate = final_state.revision_count / max(final_state.cycle_count, 1)\n",
    "print(f\"\\nRevision rate: {revision_rate:.2f} revisions/cycle\")\n",
    "\n",
    "# Quality indicator\n",
    "quality_score = final_state.current_consensus_score * approval_rate\n",
    "print(f\"\\nQuality score: {quality_score:.2f} (consensus × approval rate)\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "hegemon_env",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3.11 (hegemon_dev)",
   "language": "python",
   "name": "hegemon_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
