{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79c03994-1129-4123-84fc-528f93f57f4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/hegemon_env/lib/python3.11/site-packages/pydantic/_internal/_model_construction.py:63: UserWarning: `validate_explainability_config` overrides an existing Pydantic `@model_validator` decorator\n",
      "  warnings.warn(f'`{k}` overrides an existing Pydantic `{existing.decorator_info.decorator_repr}` decorator')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from google.cloud import secretmanager\n",
    "\n",
    "#hegemon dependencis\n",
    "from hegemon.explainability.visualizer import HeatmapGenerator\n",
    "from hegemon.config.settings import get_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30e9cc50-90ae-45d3-a075-20cf7b631a6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìç GCP Project: dark-data-discovery\n",
      "üìç Location: us-central1\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"dark-data-discovery\"\n",
    "LOCATION = \"us-central1\"\n",
    "\n",
    "print(f\"üìç GCP Project: {PROJECT_ID}\")\n",
    "print(f\"üìç Location: {LOCATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "145efff6-e199-47c6-a6e9-7a3abd60b915",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ANTHROPIC_API_KEY: **********uwAA\n"
     ]
    }
   ],
   "source": [
    "def get_secret(project_id, secret_id, version_id=\"latest\"):\n",
    "    client = secretmanager.SecretManagerServiceClient()\n",
    "    name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n",
    "    response = client.access_secret_version(request={\"name\": name})\n",
    "    return response.payload.data.decode(\"UTF-8\")\n",
    "\n",
    "try:\n",
    "    key = get_secret(PROJECT_ID, \"ANTHROPIC_API_KEY\")\n",
    "    print(f\"‚úÖ ANTHROPIC_API_KEY: {'*' * 10}{key[-4:]}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to get ANTHROPIC_API_KEY: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "793044ce-63c8-492a-8605-b14460f89684",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment configured for Vertex AI\n",
      "   Explainability enabled: true\n",
      "   Classifier model: gemini-2.5-flash\n"
     ]
    }
   ],
   "source": [
    "# Kom√≥rka 1: Setup\n",
    "# ENABLE EXPLAINABILITY (przez os.environ)\n",
    "os.environ[\"HEGEMON_EXPLAINABILITY_ENABLED\"] = \"true\"\n",
    "os.environ[\"HEGEMON_EXPLAINABILITY_SEMANTIC_FINGERPRINT\"] = \"true\"\n",
    "os.environ[\"HEGEMON_EXPLAINABILITY_CLASSIFIER_MODEL\"] = \"gemini-2.5-flash\"  # Vertex AI model\n",
    "\n",
    "# Setup logging dla Jupyter\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    force=True\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Environment configured for Vertex AI\")\n",
    "print(f\"   Explainability enabled: {os.environ.get('HEGEMON_EXPLAINABILITY_ENABLED')}\")\n",
    "print(f\"   Classifier model: {os.environ.get('HEGEMON_EXPLAINABILITY_CLASSIFIER_MODEL')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c9247d3-9fa2-4bbc-b956-60bd87240383",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-14 13:02:30,942 - hegemon.config.settings - WARNING - Explainability enabled but no Google API key found. Classifier will fail.\n",
      "2025-10-14 13:02:30,943 - hegemon.config.settings - INFO - ‚úÖ Sceptyk will use Vertex AI (Application Default Credentials)\n",
      "2025-10-14 13:02:30,944 - hegemon.config.settings - INFO - ‚úÖ All required authentication validated\n",
      "2025-10-14 13:02:30,945 - hegemon.config.settings - INFO - ‚úÖ HEGEMON Settings initialized (GCP Project: dark-data-discovery)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explainability Layers:\n",
      "  Layer 6 (Semantic): True\n",
      "  Layer 2 (Epistemic): True\n",
      "Configuration:\n",
      "  Katalizator: claude-sonnet-4-5-20250929\n",
      "  Sceptyk: gemini-2.5-pro\n",
      "  Gubernator: claude-sonnet-4-5-20250929\n",
      "  Syntezator: claude-opus-4-1-20250805\n",
      "\n",
      "Debate settings:\n",
      "  Consensus threshold: 0.7\n",
      "  Max cycles: 5\n",
      "\n",
      "‚úÖ SUCCESS: Explainability is ENABLED!\n",
      "   Proceed to next cells.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Kom√≥rka 2: Import Hegemon modules\n",
    "from hegemon.explainability.visualizer import HeatmapGenerator\n",
    "\n",
    "\n",
    "# Verify settings\n",
    "settings = get_settings()\n",
    "print(\"Explainability Layers:\")\n",
    "print(f\"  Layer 6 (Semantic): {settings.explainability_semantic_fingerprint}\")\n",
    "print(f\"  Layer 2 (Epistemic): {settings.explainability_epistemic_uncertainty}\")\n",
    "\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Katalizator: {settings.katalizator.model}\")\n",
    "print(f\"  Sceptyk: {settings.sceptyk.model}\")\n",
    "print(f\"  Gubernator: {settings.gubernator.model}\")\n",
    "print(f\"  Syntezator: {settings.syntezator.model}\")\n",
    "print(f\"\\nDebate settings:\")\n",
    "print(f\"  Consensus threshold: {settings.debate.consensus_threshold}\")\n",
    "print(f\"  Max cycles: {settings.debate.max_cycles}\")\n",
    "# CRITICAL CHECK\n",
    "if not settings.explainability_enabled:\n",
    "    print(\"\\n‚ùå ERROR: Explainability is DISABLED!\")\n",
    "    print(\"   This means environment variable was set AFTER settings were cached.\")\n",
    "    print(\"   SOLUTION: Restart kernel and run cells in correct order.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ SUCCESS: Explainability is ENABLED!\")\n",
    "    print(\"   Proceed to next cells.\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88b576cb-b965-4e8c-addb-234b7d25fa96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Mission defined:\n",
      "\n",
      "Design a comprehensive AI strategy for a mid-size e-commerce company.\n",
      "\n",
      "Context:\n",
      "- Company: 150 employees, $50M annual revenue\n",
      "- Current tech stack: Shopify + basic analytics\n",
      "- Budget: $500K annually for AI initiatives\n",
      "- Timeline: 12 months\n",
      "\n",
      "Requirements:\n",
      "1. Improve customer experience (personalization, recommendations)\n",
      "2. Reduce operational costs (automation, optimization)\n",
      "3. GDPR compliant\n",
      "4. Limited ML expertise in-house\n",
      "\n",
      "Deliverables needed:\n",
      "- Technology stack recommendations\n",
      "- Phased implementation roadmap (Q1-Q4)\n",
      "- Team structure and hiring plan\n",
      "- Risk analysis\n",
      "- Success metrics (KPIs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zdefiniuj swojƒÖ misjƒô\n",
    "MISSION = \"\"\"\n",
    "Design a comprehensive AI strategy for a mid-size e-commerce company.\n",
    "\n",
    "Context:\n",
    "- Company: 150 employees, $50M annual revenue\n",
    "- Current tech stack: Shopify + basic analytics\n",
    "- Budget: $500K annually for AI initiatives\n",
    "- Timeline: 12 months\n",
    "\n",
    "Requirements:\n",
    "1. Improve customer experience (personalization, recommendations)\n",
    "2. Reduce operational costs (automation, optimization)\n",
    "3. GDPR compliant\n",
    "4. Limited ML expertise in-house\n",
    "\n",
    "Deliverables needed:\n",
    "- Technology stack recommendations\n",
    "- Phased implementation roadmap (Q1-Q4)\n",
    "- Team structure and hiring plan\n",
    "- Risk analysis\n",
    "- Success metrics (KPIs)\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìã Mission defined:\")\n",
    "print(MISSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e63d28df-10ea-433f-9097-0a6489b4af12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Selected mode: reviewer\n",
      "\n",
      "Expected checkpoints:\n",
      "  - POST_THESIS (po ka≈ºdym Katalizatorze)\n",
      "  - POST_EVALUATION (po Gubernatorze)\n",
      "  - PRE_SYNTHESIS (przed Syntetatorem)\n"
     ]
    }
   ],
   "source": [
    "# Wybierz tryb\n",
    "MODE = \"reviewer\"  # Zmie≈Ñ na \"observer\" lub \"collaborator\"\n",
    "\n",
    "print(f\"‚úÖ Selected mode: {MODE}\")\n",
    "print(f\"\\nExpected checkpoints:\")\n",
    "if MODE == \"observer\":\n",
    "    print(\"  - PRE_SYNTHESIS (1x przed syntezƒÖ)\")\n",
    "elif MODE == \"reviewer\":\n",
    "    print(\"  - POST_THESIS (po ka≈ºdym Katalizatorze)\")\n",
    "    print(\"  - POST_EVALUATION (po Gubernatorze)\")\n",
    "    print(\"  - PRE_SYNTHESIS (przed Syntetatorem)\")\n",
    "else:\n",
    "    print(\"  - All checkpoints + detailed guidance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a8b5562-e5d7-47ad-957d-d637aabdc69d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# G≈Ç√≥wne importy HITL\n",
    "from hegemon.graph_hitl_v3 import (\n",
    "    create_hegemon_graph_hitl_v3,\n",
    "    run_debate_hitl_v3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500019b7-1851-4f26-9d6c-9550c74e5c98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); \n",
       "                    color: white; padding: 20px; border-radius: 10px; margin-bottom: 20px;'>\n",
       "            <h2 style='margin: 0;'>üîç Checkpoint: Post Thesis</h2>\n",
       "            <p style='margin: 10px 0 0 0;'>\n",
       "                <strong>Agent:</strong> Katalizator | \n",
       "                <strong>Cycle:</strong> 1 | \n",
       "                <strong>Confidence:</strong> 0.00\n",
       "            </p>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style='background: #f8f9fa; padding: 15px; border-left: 4px solid #667eea; margin-bottom: 20px;'>\n",
       "            <h3 style='margin-top: 0;'>üìã Executive Summary</h3>\n",
       "            <p>Katalizator completed analysis in cycle 1. Review the output below for key claims and concerns.</p>\n",
       "            <h4>Key Points:</h4>\n",
       "            <ul>\n",
       "                <li>Katalizator completed analysis in cycle 1</li><li>Review the output below for key claims and concerns</li>\n",
       "            </ul>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>‚ö†Ô∏è Important Claims</h3>\n",
       "            <div style='border-left: 4px solid #4CAF50; padding: 10px; margin: 10px 0; background: #f9f9f9;'>\n",
       "                <strong>Confidence: 0.80</strong> - high_impact<br>\n",
       "                <p style='margin: 5px 0;'>The fundamental insight here is that mid-size e-commerce companies don&#x27;t need to build AI infrastructure‚Äîthey need to orchestrate it intelligently. With $500K annually and limited ML expertise, the winning strategy is a &quot;composable AI stack&quot; that delivers measurable value within 90 days while buildi...</p>\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style='border: 1px solid #ddd; padding: 15px; margin: 20px 0; border-radius: 5px;'>\n",
       "            <h3>üìÑ Full Output</h3>\n",
       "            <pre style='white-space: pre-wrap; background: #f5f5f5; padding: 10px; border-radius: 5px;'>The fundamental insight here is that mid-size e-commerce companies don&#x27;t need to build AI infrastructure‚Äîthey need to orchestrate it intelligently. With $500K annually and limited ML expertise, the winning strategy is a &quot;composable AI stack&quot; that delivers measurable value within 90 days while building toward sophisticated capabilities by year-end.\n",
       "\n",
       "The core approach centers on three sequential waves of implementation. First, deploy production-ready AI APIs for immediate wins: implement a recommendation engine using existing services like Dynamic Yield or Nosto ($60K annually) to drive a targeted 15% increase in average order value, and integrate conversational AI for customer service through solutions like Ada or Intercom&#x27;s Fin ($40K annually) to handle 60% of tier-1 support queries. These tools require minimal technical lift and deliver ROI within the first quarter.\n",
       "\n",
       "Second, establish a lean data foundation by migrating from basic analytics to a modern stack combining Segment for customer data infrastructure ($30K) and a cloud data warehouse like BigQuery ($20K annually). This creates the backbone for personalization while maintaining GDPR compliance through proper data governance frameworks. Simultaneously, hire one AI product manager ($140K) and partner with a specialized implementation firm ($150K) rather than building an in-house ML team‚Äîthis provides expertise without long-term overhead.\n",
       "\n",
       "Third, by Q3, leverage the data foundation to deploy custom recommendation models using AutoML platforms like Google Vertex AI or AWS SageMaker Autopilot, targeting 25% of revenue from personalized experiences by year-end. Implement dynamic pricing optimization for inventory management, projecting 8-12% margin improvement through better stock turnover.\n",
       "\n",
       "This strategy targets $2.5M in incremental revenue and $400K in cost savings within twelve months‚Äîa 5x return on investment. The approach de-risks AI adoption through proven tools while building organizational capability for future sophistication. Success metrics include conversion rate lift, customer lifetime value growth, support ticket deflection rate, and gross margin improvement, all tracked through quarterly business reviews.</pre>\n",
       "            \n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81591ebf32344bf386f73f5dfd53f33f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>üë§ Your Feedback</h3>'), RadioButtons(description='Decision:', options=(('‚úÖ Appr‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üöÄ START DEBATE!\n",
    "print(\"=\"*80)\n",
    "print(\"ü§ñ Starting HITL Debate...\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n‚è≥ Please wait for first checkpoint...\\n\")\n",
    "\n",
    "# Run debate (INTERACTIVE - bƒôdzie czekaƒá na Tw√≥j input!)\n",
    "final_state = run_debate_hitl_v3(\n",
    "    mission=MISSION,\n",
    "    intervention_mode=MODE,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Debate Complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50084d51-c98e-4d2b-93f2-fa4870f047b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"üìä DEBATE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total cycles: {final_state.cycle_count}\")\n",
    "print(f\"Total contributions: {len(final_state.contributions)}\")\n",
    "print(f\"Human feedbacks provided: {len(final_state.human_feedback)}\")\n",
    "print(f\"Revisions requested: {final_state.revision_count}\")\n",
    "print(f\"Final consensus score: {final_state.current_consensus_score:.2f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212672b9-6511-4aeb-9bfc-10667b4c08ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedback breakdown\n",
    "if final_state.human_feedback:\n",
    "    print(\"üìù YOUR FEEDBACK HISTORY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, feedback in enumerate(final_state.human_feedback, 1):\n",
    "        print(f\"\\n{i}. Checkpoint: {feedback.checkpoint.value}\")\n",
    "        print(f\"   Decision: {feedback.decision.value}\")\n",
    "        print(f\"   Timestamp: {feedback.timestamp.strftime('%H:%M:%S')}\")\n",
    "        \n",
    "        if feedback.guidance:\n",
    "            print(f\"   Your guidance: {feedback.guidance[:100]}...\")\n",
    "        \n",
    "        if feedback.priority_claims:\n",
    "            print(f\"   Priority claims: {len(feedback.priority_claims)}\")\n",
    "        \n",
    "        if feedback.flagged_concerns:\n",
    "            print(f\"   Flagged concerns: {len(feedback.flagged_concerns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b776096-d8e8-416e-b003-d66bd3d4f228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Plan overview\n",
    "if final_state.final_plan:\n",
    "    print(\"\\nüìã FINAL PLAN OVERVIEW\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    plan = final_state.final_plan\n",
    "    \n",
    "    print(f\"\\nMission: {plan.mission_overview[:200]}...\")\n",
    "    print(f\"\\nRequired Agents: {len(plan.required_agents)}\")\n",
    "    for agent in plan.required_agents[:5]:  # Show first 5\n",
    "        print(f\"  - {agent.role}: {', '.join(agent.skills[:3])}\")\n",
    "    \n",
    "    print(f\"\\nWorkflow Steps: {len(plan.workflow)}\")\n",
    "    for step in plan.workflow[:5]:  # Show first 5\n",
    "        print(f\"  {step.step_number}. {step.description[:60]}...\")\n",
    "        print(f\"     Agent: {step.assigned_agent} | Duration: {step.estimated_duration}\")\n",
    "    \n",
    "    print(f\"\\nTotal workflow duration: {plan.total_estimated_duration}\")\n",
    "    print(f\"Risk score: {plan.overall_risk_score:.2f}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No final plan generated (debate may have been rejected)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128a1bc7-ac60-44a9-960d-cd6da58eb0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to JSON\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "output_dir = Path(\"output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = output_dir / f\"hegemon_hitl_{MODE}_{timestamp}.json\"\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(\n",
    "        final_state.model_dump(mode='json'),\n",
    "        f,\n",
    "        indent=2,\n",
    "        ensure_ascii=False,\n",
    "    )\n",
    "\n",
    "print(f\"üíæ Results saved to: {output_file}\")\n",
    "print(f\"   File size: {output_file.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf68881-2339-415c-b264-37f378e9eb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all agent contributions\n",
    "print(\"üîç FULL DEBATE HISTORY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for contrib in final_state.contributions:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Agent: {contrib.agent_id} | Type: {contrib.type} | Cycle: {contrib.cycle}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\n{contrib.content[:500]}...\")\n",
    "    print(f\"\\nRationale: {contrib.rationale[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d751311-8b0a-4088-9507-cb7f41feea64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "from collections import Counter\n",
    "\n",
    "print(\"üìà PERFORMANCE METRICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Intervention metrics\n",
    "intervention_rate = len(final_state.human_feedback) / max(final_state.cycle_count, 1)\n",
    "print(f\"\\nIntervention rate: {intervention_rate:.2f} feedbacks/cycle\")\n",
    "\n",
    "# Decision breakdown\n",
    "if final_state.human_feedback:\n",
    "    decisions = Counter(f.decision.value for f in final_state.human_feedback)\n",
    "    total = len(final_state.human_feedback)\n",
    "    \n",
    "    print(f\"\\nDecision breakdown:\")\n",
    "    for decision, count in decisions.items():\n",
    "        percentage = (count / total) * 100\n",
    "        print(f\"  {decision}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    approval_rate = decisions.get('approve', 0) / total\n",
    "    print(f\"\\nApproval rate: {approval_rate:.1%}\")\n",
    "\n",
    "# Revision metrics\n",
    "revision_rate = final_state.revision_count / max(final_state.cycle_count, 1)\n",
    "print(f\"\\nRevision rate: {revision_rate:.2f} revisions/cycle\")\n",
    "\n",
    "# Quality indicator\n",
    "quality_score = final_state.current_consensus_score * approval_rate\n",
    "print(f\"\\nQuality score: {quality_score:.2f} (consensus √ó approval rate)\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "hegemon_env",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3.11 (hegemon_dev)",
   "language": "python",
   "name": "hegemon_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
