{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3126fe7-bc63-4797-b7e6-d490ca636d9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bce2f0-1cc2-46d9-b443-d0061c19fcff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "PROJECT_ID = \"dark-data-discovery\"\n",
    "LOCATION = \"us-central1\"\n",
    "\n",
    "print(f\"üìç GCP Project: {PROJECT_ID}\")\n",
    "print(f\"üìç Location: {LOCATION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6debd960-7fc2-4dbe-b4d3-13c8fc609d58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import secretmanager\n",
    "\n",
    "def get_secret(project_id, secret_id, version_id=\"latest\"):\n",
    "    client = secretmanager.SecretManagerServiceClient()\n",
    "    name = f\"projects/{project_id}/secrets/{secret_id}/versions/{version_id}\"\n",
    "    response = client.access_secret_version(request={\"name\": name})\n",
    "    return response.payload.data.decode(\"UTF-8\")\n",
    "\n",
    "try:\n",
    "    key = get_secret(PROJECT_ID, \"ANTHROPIC_API_KEY\")\n",
    "    print(f\"‚úÖ ANTHROPIC_API_KEY: {'*' * 10}{key[-4:]}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to get ANTHROPIC_API_KEY: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bf7c89-92bb-4c40-9045-a17b8177f1bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from hegemon import create_hegemon_graph, get_settings, DebateState, FinalPlan\n",
    "\n",
    "print(\"‚úÖ HEGEMON imported!\")\n",
    "print(f\"Version: {__import__('hegemon').__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ff6b2a-133c-414d-b32a-f092284ae1d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "settings = get_settings()\n",
    "\n",
    "print(f\"‚úÖ Settings initialized\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Project: {settings.gcp_project_id}\")\n",
    "print(f\"  Location: {settings.gcp_location}\")\n",
    "print(f\"  Katalizator: {settings.katalizator.model} ({settings.katalizator.provider})\")\n",
    "print(f\"  Sceptyk: {settings.sceptyk.model} ({settings.sceptyk.provider}, Vertex AI: {settings.sceptyk.use_vertex_ai})\")\n",
    "print(f\"  Gubernator: {settings.gubernator.model} ({settings.gubernator.provider})\")\n",
    "print(f\"  Syntezator: {settings.syntezator.model} ({settings.syntezator.provider})\")\n",
    "print(f\"\\nDebate Config:\")\n",
    "print(f\"  Consensus threshold: {settings.debate.consensus_threshold}\")\n",
    "print(f\"  Max cycles: {settings.debate.max_cycles}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4737b9e7-6597-41c0-9818-2ee5b91f39e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph = create_hegemon_graph()\n",
    "\n",
    "print(\"‚úÖ HEGEMON graph created!\")\n",
    "print(f\"\\nGraph structure:\")\n",
    "print(f\"  Nodes: {list(graph.nodes.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c239a264-7bdd-4d48-8596-d103ddf08a6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MISSION = \"\"\"\n",
    "Design automated book layout pipeline using InDesign scripting for 280-page \n",
    "technical programming book, executable by AI agents with access to Adobe InDesign API, \n",
    "Python scripting, and document processing tools.\n",
    "\n",
    "BOOK SPECIFICATIONS:\n",
    "- Title: \"Advanced Python Patterns: A Practitioner's Guide\"\n",
    "- Content: 12 chapters, 280 pages (pre-formatted manuscript in Markdown/LaTeX)\n",
    "- Format: 7\" x 10\" trade paperback (Crown Quarto)\n",
    "- Output: Print-ready PDF (IngramSpark specs) + ePub\n",
    "- Audience: professional software engineers\n",
    "\n",
    "CONTENT COMPLEXITY:\n",
    "- 50 code blocks (10-200 lines each, Python syntax highlighting)\n",
    "- 30 technical diagrams (provided as SVG/PNG, 300 DPI)\n",
    "- Mathematical notation (LaTeX format in source)\n",
    "- 3 callout box types with structured markup\n",
    "- Index: 500+ entries (extractable from source annotations)\n",
    "- Bibliography: 120 citations (BibTeX format provided)\n",
    "\n",
    "EXECUTION CONSTRAINTS (AI-OPTIMIZED):\n",
    "- Timeline: Complete pipeline execution in <24 hours wall-clock time\n",
    "- Compute budget: $50 total (API calls, cloud compute)\n",
    "- Tools available to agents:\n",
    "  * InDesign Server API (scripting via ExtendScript/JavaScript)\n",
    "  * Python 3.11+ with libraries: reportlab, PyPDF2, lxml, beautifulsoup4\n",
    "  * Pandoc for format conversion (Markdown ‚Üí ICML)\n",
    "  * Pygments for syntax highlighting (pre-processing step)\n",
    "  * Adobe Preflight API for PDF/X validation\n",
    "  * PAC 2024 for WCAG accessibility checking\n",
    "- Manuscript format: Structured Markdown with frontmatter (YAML metadata)\n",
    "- No human-in-loop approvals (fully automated validation)\n",
    "\n",
    "AUTOMATION REQUIREMENTS:\n",
    "- Template generation: Programmatic master page creation via InDesign scripting\n",
    "- Style application: Paragraph/character styles applied via GREP and scripting\n",
    "- Code highlighting: Pre-process with Pygments, import as styled text\n",
    "- Figure placement: Automated anchoring based on markup (e.g., `![Figure 3.2](path/to/diagram.svg)`)\n",
    "- Index generation: Extract from `\\index{term}` tags in source, auto-generate with InDesign API\n",
    "- Cross-references: Automated figure/table numbering via InDesign cross-ref system\n",
    "- Accessibility tagging: Automated structure tags based on semantic HTML export from Pandoc\n",
    "\n",
    "DELIVERABLES (ALL PROGRAMMATICALLY GENERATED):\n",
    "- InDesign template (.indt) created via scripting\n",
    "- Print-ready PDF/X-1a passing Adobe Preflight (0 errors)\n",
    "- Tagged PDF passing PAC accessibility check (WCAG 2.1 AA)\n",
    "- ePub 3.0 with reflowable layout\n",
    "- Automated validation report (JSON format with all quality metrics)\n",
    "- Reproducible pipeline (Docker container or shell script)\n",
    "\n",
    "SUCCESS METRICS (AUTOMATED VALIDATION):\n",
    "- IngramSpark preflight: PASS with 0 critical errors\n",
    "- Accessibility check: PASS with 0 WCAG 2.1 AA violations\n",
    "- Code font legibility: 9pt minimum (validated via PDF text extraction)\n",
    "- Index accuracy: Auto-generated, cross-checked against source tags\n",
    "- Build reproducibility: Pipeline runs identically on re-execution\n",
    "- Processing time: <4 hours for full 280-page layout\n",
    "- API cost: <$50 (primarily InDesign Server API calls + LLM tool use)\n",
    "\n",
    "TECHNICAL SPECIFICATIONS:\n",
    "- Print: 300 DPI, CMYK (Coated FOGRA39), 0.125\" bleed, PDF/X-1a:2001\n",
    "- Typography: \n",
    "  * Body: Minion Pro 10.75pt (embedded subset)\n",
    "  * Headings: Myriad Pro (4 levels: 24pt, 18pt, 14pt, 12pt)\n",
    "  * Code: Source Code Pro 9pt (monospace)\n",
    "- Color: Black + Pantone 2738 C (accent - converted to CMYK for print)\n",
    "- Page geometry: 7\" √ó 10\", margins (inner: 0.75\", outer: 1.5\", top: 0.75\", bottom: 1\")\n",
    "\n",
    "AGENT CAPABILITIES ASSUMED:\n",
    "- Code execution: Python, JavaScript (ExtendScript)\n",
    "- File operations: Read/write local filesystem, download from URLs\n",
    "- API calls: REST APIs (InDesign Server, Preflight services)\n",
    "- Document parsing: Markdown, LaTeX, YAML, XML (ICML), JSON\n",
    "- Text processing: Regex, GREP patterns, string manipulation\n",
    "- Validation: PDF analysis, accessibility tree inspection\n",
    "\n",
    "FAILURE MODES TO HANDLE:\n",
    "- Invalid source markup ‚Üí Validation error with line numbers\n",
    "- Missing diagrams ‚Üí Placeholder with error message in output\n",
    "- Font embedding issues ‚Üí Fallback to safe font list\n",
    "- CMYK conversion problems ‚Üí Use sRGB with warning flag\n",
    "- Preflight failures ‚Üí Return detailed error report for manual review\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "Execution plan must specify:\n",
    "1. Agent roles with specific tools assigned\n",
    "2. Workflow as DAG with parallelizable steps\n",
    "3. Automated validation checkpoints (not human approval)\n",
    "4. Error handling and retry logic\n",
    "5. Cost estimation per step (API calls, compute time)\n",
    "6. Final deliverable verification (checksums, validation reports)\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìã Mission defined:\")\n",
    "print(MISSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be7cda2-129b-428e-9454-c2e96c57309d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_state = {\n",
    "    \"mission\": MISSION,\n",
    "    \"contributions\": [],\n",
    "    \"cycle_count\": 1,\n",
    "    \"current_consensus_score\": 0.0,\n",
    "    \"final_plan\": None,\n",
    "}\n",
    "\n",
    "print(\"üöÄ Starting HEGEMON debate...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "final_state = graph.invoke(initial_state)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ Debate complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb527d1-865b-4159-b79c-5ba6ce2e4299",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä DEBATE SUMMARY\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "print(f\"Total cycles: {final_state['cycle_count']}\")\n",
    "print(f\"Final consensus score: {final_state['current_consensus_score']:.2f}\")\n",
    "print(f\"Total contributions: {len(final_state['contributions'])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìù DEBATE HISTORY\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "for contrib in final_state['contributions']:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Agent: {contrib.agent_id} | Type: {contrib.type} | Cycle: {contrib.cycle}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\n{contrib.content[:500]}...\")\n",
    "    print(f\"\\nRationale: {contrib.rationale}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053b01a6-2e6b-4955-b24d-82fe1503e1ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_plan = final_state['final_plan']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéØ FINAL STRATEGIC PLAN\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "print(\"üìã MISSION OVERVIEW:\")\n",
    "print(\"-\" * 80)\n",
    "print(final_plan.mission_overview)\n",
    "\n",
    "print(\"\\n\\nüë• REQUIRED AGENTS:\")\n",
    "print(\"-\" * 80)\n",
    "for agent in final_plan.required_agents:\n",
    "    print(f\"\\n{agent.role}\")\n",
    "    print(f\"  Description: {agent.description}\")\n",
    "    print(f\"  Skills: {', '.join(agent.required_skills)}\")\n",
    "\n",
    "print(\"\\n\\nüìä WORKFLOW ({} steps):\".format(len(final_plan.workflow)))\n",
    "print(\"-\" * 80)\n",
    "for step in final_plan.workflow:\n",
    "    print(f\"\\nStep {step.step_id}: {step.assigned_agent_role}\")\n",
    "    print(f\"  Task: {step.description}\")\n",
    "    if step.dependencies:\n",
    "        print(f\"  Dependencies: {step.dependencies}\")\n",
    "\n",
    "print(\"\\n\\n‚ö†Ô∏è RISK ANALYSIS:\")\n",
    "print(\"-\" * 80)\n",
    "print(final_plan.risk_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eb3060-c723-4ffe-9907-ed7a3677e818",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 13: Export to JSON\n",
    "# ============================================================================\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "output = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"mission\": final_state['mission'],\n",
    "    \"debate_summary\": {\n",
    "        \"total_cycles\": final_state['cycle_count'],\n",
    "        \"final_consensus_score\": final_state['current_consensus_score'],\n",
    "        \"total_contributions\": len(final_state['contributions']),\n",
    "    },\n",
    "    \"debate_history\": [\n",
    "        {\n",
    "            \"agent_id\": c.agent_id,\n",
    "            \"type\": c.type,\n",
    "            \"cycle\": c.cycle,\n",
    "            \"content\": c.content,\n",
    "            \"rationale\": c.rationale,\n",
    "        }\n",
    "        for c in final_state['contributions']\n",
    "    ],\n",
    "    \"final_plan\": {\n",
    "        \"mission_overview\": final_plan.mission_overview,\n",
    "        \"required_agents\": [\n",
    "            {\n",
    "                \"role\": a.role,\n",
    "                \"description\": a.description,\n",
    "                \"required_skills\": a.required_skills,\n",
    "            }\n",
    "            for a in final_plan.required_agents\n",
    "        ],\n",
    "        \"workflow\": [\n",
    "            {\n",
    "                \"step_id\": s.step_id,\n",
    "                \"description\": s.description,\n",
    "                \"assigned_agent_role\": s.assigned_agent_role,\n",
    "                \"dependencies\": s.dependencies,\n",
    "            }\n",
    "            for s in final_plan.workflow\n",
    "        ],\n",
    "        \"risk_analysis\": final_plan.risk_analysis,\n",
    "    }\n",
    "}\n",
    "\n",
    "output_path = Path(\"output\")\n",
    "output_path.mkdir(exist_ok=True)\n",
    "\n",
    "filename = output_path / f\"hegemon_output_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ Results exported to: {filename}\")\n",
    "print(f\"üìä File size: {filename.stat().st_size / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0abedf3-e007-4c19-8fe6-01023689f923",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 14: Visualize Workflow as Graph (Optional)\n",
    "# ============================================================================\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import networkx as nx\n",
    "    \n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    for step in final_plan.workflow:\n",
    "        G.add_node(step.step_id, label=f\"Step {step.step_id}\\n{step.assigned_agent_role}\")\n",
    "        for dep in step.dependencies:\n",
    "            G.add_edge(dep, step.step_id)\n",
    "    \n",
    "    plt.figure(figsize=(14, 10))\n",
    "    pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "    \n",
    "    nx.draw(G, pos, \n",
    "            node_color='lightblue', \n",
    "            node_size=3000,\n",
    "            with_labels=True,\n",
    "            labels={n: f\"Step {n}\" for n in G.nodes()},\n",
    "            font_size=10,\n",
    "            font_weight='bold',\n",
    "            arrows=True,\n",
    "            arrowsize=20,\n",
    "            edge_color='gray',\n",
    "            width=2)\n",
    "    \n",
    "    plt.title(\"Workflow Dependency Graph\", fontsize=16, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path / \"workflow_graph.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Workflow graph saved!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è matplotlib/networkx not installed. Skipping visualization.\")\n",
    "    print(\"Install with: pip install matplotlib networkx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b84016-b2fe-4de3-80bf-c40049149b89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 15: Quick Stats\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìà QUICK STATS\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "print(f\"Mission length: {len(MISSION)} characters\")\n",
    "print(f\"Debate cycles: {final_state['cycle_count']}\")\n",
    "print(f\"Final consensus: {final_state['current_consensus_score']:.2%}\")\n",
    "print(f\"Total contributions: {len(final_state['contributions'])}\")\n",
    "print(f\"  - Theses: {sum(1 for c in final_state['contributions'] if c.type == 'Thesis')}\")\n",
    "print(f\"  - Antitheses: {sum(1 for c in final_state['contributions'] if c.type == 'Antithesis')}\")\n",
    "print(f\"  - Evaluations: {sum(1 for c in final_state['contributions'] if c.type == 'Evaluation')}\")\n",
    "print(f\"  - Final Plans: {sum(1 for c in final_state['contributions'] if c.type == 'FinalPlan')}\")\n",
    "\n",
    "print(f\"\\nFinal Plan:\")\n",
    "print(f\"  - Required agents: {len(final_plan.required_agents)}\")\n",
    "print(f\"  - Workflow steps: {len(final_plan.workflow)}\")\n",
    "print(f\"  - Mission overview length: {len(final_plan.mission_overview)} chars\")\n",
    "print(f\"  - Risk analysis length: {len(final_plan.risk_analysis)} chars\")\n",
    "\n",
    "total_chars = sum(len(c.content) for c in final_state['contributions'])\n",
    "print(f\"\\nTotal debate output: {total_chars:,} characters ({total_chars/1024:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e887b2b0-d7c8-437a-97f5-e2950fd06d52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e953b6b-6866-468c-8e76-c9d28589ffb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25d55f6c-ab78-478c-8bc4-77ad12b556d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment configured for Vertex AI\n",
      "   Explainability enabled: true\n",
      "   Classifier model: gemini-2.5-flash\n"
     ]
    }
   ],
   "source": [
    "# Kom√≥rka 1: Setup\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# ENABLE EXPLAINABILITY (przez os.environ)\n",
    "os.environ[\"HEGEMON_EXPLAINABILITY_ENABLED\"] = \"true\"\n",
    "os.environ[\"HEGEMON_EXPLAINABILITY_SEMANTIC_FINGERPRINT\"] = \"true\"\n",
    "os.environ[\"HEGEMON_EXPLAINABILITY_CLASSIFIER_MODEL\"] = \"gemini-2.5-flash\"  # Vertex AI model\n",
    "\n",
    "# Setup logging dla Jupyter\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    force=True\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Environment configured for Vertex AI\")\n",
    "print(f\"   Explainability enabled: {os.environ.get('HEGEMON_EXPLAINABILITY_ENABLED')}\")\n",
    "print(f\"   Classifier model: {os.environ.get('HEGEMON_EXPLAINABILITY_CLASSIFIER_MODEL')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7b6c21c-454f-474f-94b9-9dc5d9e9f7db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-03 11:34:04,315 - hegemon.config.settings - INFO - ‚úÖ ANTHROPIC_API_KEY loaded from Secret Manager\n",
      "2025-10-03 11:34:04,317 - hegemon.config.settings - INFO - ‚úÖ Google Gemini will use Vertex AI (ADC authentication)\n",
      "2025-10-03 11:34:04,385 - hegemon.config.settings - INFO - ‚úÖ OPENAI_API_KEY loaded from Secret Manager\n",
      "/opt/conda/envs/hegemon_env/lib/python3.11/site-packages/pydantic/_internal/_model_construction.py:63: UserWarning: `validate_explainability_config` overrides an existing Pydantic `@model_validator` decorator\n",
      "  warnings.warn(f'`{k}` overrides an existing Pydantic `{existing.decorator_info.decorator_repr}` decorator')\n",
      "2025-10-03 11:34:08,314 - hegemon.config.settings - WARNING - Explainability enabled but no Google API key found. Classifier will fail.\n",
      "2025-10-03 11:34:08,315 - hegemon.config.settings - INFO - ‚úÖ Sceptyk will use Vertex AI (Application Default Credentials)\n",
      "2025-10-03 11:34:08,316 - hegemon.config.settings - INFO - ‚úÖ All required authentication validated\n",
      "2025-10-03 11:34:08,317 - hegemon.config.settings - INFO - ‚úÖ HEGEMON Settings initialized (GCP Project: dark-data-discovery)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explainability Layers:\n",
      "  Layer 6 (Semantic): True\n",
      "  Layer 2 (Epistemic): True\n",
      "================================================================================\n",
      "‚úÖ STEP 2: Settings loaded and verified\n",
      "================================================================================\n",
      "GCP Project ID: dark-data-discovery\n",
      "GCP Location: us-central1\n",
      "\n",
      "üîç EXPLAINABILITY SETTINGS:\n",
      "   enabled: True\n",
      "   semantic_fingerprint: True\n",
      "   classifier_model: gemini-2.0-flash\n",
      "   cache_size: 1000\n",
      "================================================================================\n",
      "\n",
      "‚úÖ SUCCESS: Explainability is ENABLED!\n",
      "   Proceed to next cells.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Kom√≥rka 2: Import Hegemon modules\n",
    "from hegemon.graph import create_hegemon_graph\n",
    "from hegemon.explainability.visualizer import HeatmapGenerator\n",
    "from hegemon.config.settings import get_settings\n",
    "\n",
    "# Verify settings\n",
    "settings = get_settings()\n",
    "print(\"Explainability Layers:\")\n",
    "print(f\"  Layer 6 (Semantic): {settings.explainability_semantic_fingerprint}\")\n",
    "print(f\"  Layer 2 (Epistemic): {settings.explainability_epistemic_uncertainty}\")\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ STEP 2: Settings loaded and verified\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"GCP Project ID: {settings.gcp_project_id}\")\n",
    "print(f\"GCP Location: {settings.gcp_location}\")\n",
    "print()\n",
    "print(\"üîç EXPLAINABILITY SETTINGS:\")\n",
    "print(f\"   enabled: {settings.explainability_enabled}\")\n",
    "print(f\"   semantic_fingerprint: {settings.explainability_semantic_fingerprint}\")\n",
    "print(f\"   classifier_model: {settings.explainability_classifier_model}\")\n",
    "print(f\"   cache_size: {settings.explainability_cache_size}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# CRITICAL CHECK\n",
    "if not settings.explainability_enabled:\n",
    "    print(\"\\n‚ùå ERROR: Explainability is DISABLED!\")\n",
    "    print(\"   This means environment variable was set AFTER settings were cached.\")\n",
    "    print(\"   SOLUTION: Restart kernel and run cells in correct order.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ SUCCESS: Explainability is ENABLED!\")\n",
    "    print(\"   Proceed to next cells.\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41d6d016-2a0e-4815-9d68-b201dd6d9d97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mission defined\n",
      "================================================================================\n",
      "\n",
      "Design a comprehensive AI-powered customer service automation strategy \n",
      "for a mid-sized e-commerce company (500 employees, $50M annual revenue).\n",
      "\n",
      "Requirements:\n",
      "- Reduce customer service costs by 30%\n",
      "- Maintain or improve customer satisfaction scores\n",
      "- Implementation timeline: 6 months\n",
      "- Budget: $500,000\n",
      "- Must integrate with existing Salesforce CRM\n",
      "\n",
      "Deliverable: Detailed implementation plan with agent roles, workflow, \n",
      "and risk analysis.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Kom√≥rka 3: Define mission\n",
    "mission = \"\"\"\n",
    "Design a comprehensive AI-powered customer service automation strategy \n",
    "for a mid-sized e-commerce company (500 employees, $50M annual revenue).\n",
    "\n",
    "Requirements:\n",
    "- Reduce customer service costs by 30%\n",
    "- Maintain or improve customer satisfaction scores\n",
    "- Implementation timeline: 6 months\n",
    "- Budget: $500,000\n",
    "- Must integrate with existing Salesforce CRM\n",
    "\n",
    "Deliverable: Detailed implementation plan with agent roles, workflow, \n",
    "and risk analysis.\n",
    "\"\"\"\n",
    "\n",
    "# initial_state = {\n",
    "#     \"mission\": mission,\n",
    "#     \"contributions\": [],\n",
    "#     \"cycle_count\": 1,\n",
    "#     \"current_consensus_score\": 0.0,\n",
    "#     \"final_plan\": None,\n",
    "# }\n",
    "\n",
    "\n",
    "# U≈ºyj reviewer mode\n",
    "initial_state = {\n",
    "    \"mission\": \"Zaprojektuj ML pipeline\",\n",
    "    \"contributions\": [],\n",
    "    \"cycle_count\": 1,\n",
    "    \"current_consensus_score\": 0.0,\n",
    "    \"final_plan\": None,\n",
    "    \"intervention_mode\": \"reviewer\",  # W≈ÇƒÖcz checkpoints\n",
    "    \"current_checkpoint\": None,\n",
    "    \"human_feedback_history\": [],\n",
    "    \"paused_at\": None,\n",
    "    \"revision_count_per_checkpoint\": {},\n",
    "    \"checkpoint_snapshots\": {}\n",
    "}\n",
    "\n",
    "\n",
    "print(\"‚úÖ Mission defined\")\n",
    "print(\"=\" * 80)\n",
    "print(mission)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d86747ad-326b-44ac-a793-eeddb163afb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from hegemon.graph_hitl import create_hegemon_graph_hitl\n",
    "from hegemon.hitl import HumanFeedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f02a1e1f-d2f4-4a9e-bfb1-289ec0aee1f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-03 11:37:22,077 - hegemon.graph_hitl - INFO - üèóÔ∏è Building HEGEMON graph with HITL checkpoints...\n",
      "2025-10-03 11:37:22,101 - hegemon.graph_hitl - INFO - ‚úÖ HEGEMON graph with HITL compiled successfully!\n",
      "2025-10-03 11:37:22,101 - hegemon.graph_hitl - INFO -    Checkpoints: post_thesis, post_evaluation, pre_synthesis\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Hegemon graph created\n",
      "   Nodes: ['__start__', 'katalizator', 'sceptyk', 'gubernator', 'syntezator', 'increment_cycle', 'checkpoint_post_thesis', 'checkpoint_post_evaluation', 'checkpoint_pre_synthesis']\n"
     ]
    }
   ],
   "source": [
    "# Kom√≥rka 4: Create Hegemon graph\n",
    "graph = create_hegemon_graph_hitl()\n",
    "\n",
    "print(\"‚úÖ Hegemon graph created\")\n",
    "print(f\"   Nodes: {list(graph.nodes.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4340d29b-0f8c-4e8d-b98a-8ca5d9eae7b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Kom√≥rka 5: Run debate\n",
    "print(\"üé≠ Starting dialectical debate...\")\n",
    "print(\"‚è≥ This will take 2-5 minutes...\\n\")\n",
    "\n",
    "final_state = graph.invoke(initial_state)\n",
    "\n",
    "\n",
    "# Auto-export Layer 2 data\n",
    "from hegemon.explainability.visualizer import (\n",
    "    export_all_epistemic_profiles,\n",
    "    create_epistemic_comparison_report,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXPORTING LAYER 2 DATA...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "exported = export_all_epistemic_profiles(\n",
    "    final_state[\"contributions\"],\n",
    "    output_dir=\"epistemic_exports\",\n",
    "    format=\"both\"\n",
    ")\n",
    "\n",
    "report = create_epistemic_comparison_report(\n",
    "    final_state[\"contributions\"],\n",
    "    output_path=\"epistemic_comparison.md\"\n",
    ")\n",
    "\n",
    "\n",
    "def export_hitl_feedback_summary(state: dict, output_path: str = \"hitl_summary.md\") -> str:\n",
    "    \"\"\"\n",
    "    Generate markdown report of human interventions.\n",
    "    \n",
    "    Shows:\n",
    "    - Which checkpoints were reached\n",
    "    - What decisions were made (approve/revise/reject)\n",
    "    - What guidance was provided\n",
    "    - How many revisions per checkpoint\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    feedback_history = state.get(\"human_feedback_history\", [])\n",
    "    revision_counts = state.get(\"revision_count_per_checkpoint\", {})\n",
    "    checkpoints_reached = state.get(\"checkpoint_snapshots\", {})\n",
    "    \n",
    "    lines = [\n",
    "        \"# HEGEMON Human-in-the-Loop Summary\",\n",
    "        \"\",\n",
    "        f\"**Intervention Mode:** {state['intervention_mode']}\",\n",
    "        f\"**Total Feedback Submissions:** {len(feedback_history)}\",\n",
    "        f\"**Checkpoints Reached:** {len(checkpoints_reached)}\",\n",
    "        \"\",\n",
    "        \"---\",\n",
    "        \"\",\n",
    "    ]\n",
    "    \n",
    "    if feedback_history:\n",
    "        lines.extend([\n",
    "            \"## üìù Feedback History\",\n",
    "            \"\",\n",
    "        ])\n",
    "        \n",
    "        for i, fb in enumerate(feedback_history, 1):\n",
    "            lines.extend([\n",
    "                f\"### Feedback #{i}\",\n",
    "                f\"- **Checkpoint:** {fb.get('checkpoint', 'unknown')}\",\n",
    "                f\"- **Decision:** {fb.get('decision', 'unknown')}\",\n",
    "                f\"- **Timestamp:** {fb.get('timestamp', 'unknown')}\",\n",
    "                \"\",\n",
    "            ])\n",
    "            \n",
    "            if fb.get('guidance'):\n",
    "                lines.extend([\n",
    "                    f\"**Guidance:**\",\n",
    "                    f\"> {fb['guidance']}\",\n",
    "                    \"\",\n",
    "                ])\n",
    "            \n",
    "            if fb.get('priority_claims'):\n",
    "                lines.append(\"**Priority Claims:**\")\n",
    "                for claim in fb['priority_claims']:\n",
    "                    lines.append(f\"- {claim}\")\n",
    "                lines.append(\"\")\n",
    "            \n",
    "            if fb.get('flagged_concerns'):\n",
    "                lines.append(\"**Flagged Concerns:**\")\n",
    "                for concern in fb['flagged_concerns']:\n",
    "                    lines.append(f\"- {concern}\")\n",
    "                lines.append(\"\")\n",
    "            \n",
    "            lines.append(\"---\")\n",
    "            lines.append(\"\")\n",
    "    else:\n",
    "        lines.extend([\n",
    "            \"## No Human Feedback\",\n",
    "            \"\",\n",
    "            \"Debate ran in **observer mode** or no interventions were made.\",\n",
    "            \"\",\n",
    "        ])\n",
    "    \n",
    "    if revision_counts:\n",
    "        lines.extend([\n",
    "            \"## üîÑ Revision Statistics\",\n",
    "            \"\",\n",
    "        ])\n",
    "        \n",
    "        for checkpoint, count in revision_counts.items():\n",
    "            lines.append(f\"- **{checkpoint}:** {count} revision(s)\")\n",
    "        \n",
    "        lines.append(\"\")\n",
    "    \n",
    "    content = \"\\n\".join(lines)\n",
    "    \n",
    "    Path(output_path).write_text(content, encoding=\"utf-8\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Export HITL summary\n",
    "hitl_report = export_hitl_feedback_summary(\n",
    "    final_state,\n",
    "    output_path=\"hitl_summary.md\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Exported {len(exported['json'])} epistemic profiles\")\n",
    "print(f\"‚úÖ Layer 2 Report: {report}\")\n",
    "print(f\"‚úÖ HITL Summary: {hitl_report}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"‚úÖ Debate completed in {final_state['cycle_count']} cycles\")\n",
    "print(f\"   Final consensus: {final_state.get('current_consensus_score', 0.0):.2f}\")\n",
    "print(f\"   Total contributions: {len(final_state['contributions'])}\")\n",
    "print(f\"   Human interventions: {len(final_state['human_feedback_history'])}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "print(f\"‚úÖ Exported {len(exported['json'])} profiles\")\n",
    "print(f\"‚úÖ Report: {report}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# BONUS: HITL-specific metrics (NEW!)\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"   Intervention mode: {final_state['intervention_mode']}\")\n",
    "print(f\"   Human feedback count: {len(final_state['human_feedback_history'])}\")\n",
    "print(f\"   Checkpoints reached: {len(final_state['checkpoint_snapshots'])}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5c03c4-ca8c-4537-823e-82abae9aa4ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Kom√≥rka 6: Inspect contributions\n",
    "print(\"üìã DEBATE CONTRIBUTIONS:\\n\")\n",
    "\n",
    "for i, contrib in enumerate(final_state[\"contributions\"], 1):\n",
    "    print(f\"{i}. {contrib.agent_id} (Cycle {contrib.cycle}) - {contrib.type}\")\n",
    "    print(f\"   Content length: {len(contrib.content)} chars\")\n",
    "    \n",
    "    # Check if has explainability\n",
    "    if contrib.explainability and contrib.explainability.semantic_fingerprint:\n",
    "        vector = contrib.explainability.semantic_fingerprint\n",
    "        print(f\"   ‚úÖ Explainability: {vector.processing_time_ms}ms, Cache: {vector.cache_hit}\")\n",
    "        \n",
    "        # Show top 3 concepts\n",
    "        top_3 = vector.top_k(3)\n",
    "        print(f\"   Top concepts: {', '.join([f'{cid}({score:.2f})' for cid, score in top_3])}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå No explainability data\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62c296c-873b-44d4-b0f0-48b7de14864a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Kom√≥rka 7: Visualize semantic fingerprints\n",
    "visualizer = HeatmapGenerator()\n",
    "\n",
    "print(\"üìä SEMANTIC FINGERPRINTS\\n\")\n",
    "\n",
    "for contrib in final_state[\"contributions\"]:\n",
    "    if contrib.explainability and contrib.explainability.semantic_fingerprint:\n",
    "        vector = contrib.explainability.semantic_fingerprint\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(f\"{contrib.agent_id} - Cycle {contrib.cycle} ({contrib.type})\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        heatmap = visualizer.generate_text_heatmap(vector, top_k=10)\n",
    "        print(heatmap)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8944d4a7-fb5c-47d9-b596-8a5888deeeba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Kom√≥rka 8: Compare Katalizator vs Sceptyk\n",
    "kataliz_contribs = [c for c in final_state[\"contributions\"] if c.agent_id == \"Katalizator\"]\n",
    "sceptyk_contribs = [c for c in final_state[\"contributions\"] if c.agent_id == \"Sceptyk\"]\n",
    "\n",
    "if kataliz_contribs and sceptyk_contribs:\n",
    "    kataliz_last = kataliz_contribs[-1]\n",
    "    sceptyk_last = sceptyk_contribs[-1]\n",
    "    \n",
    "    if (kataliz_last.explainability and sceptyk_last.explainability and\n",
    "        kataliz_last.explainability.semantic_fingerprint and \n",
    "        sceptyk_last.explainability.semantic_fingerprint):\n",
    "        \n",
    "        print(\"üîÑ COMPARISON: Katalizator vs Sceptyk\\n\")\n",
    "        \n",
    "        comparison = visualizer.generate_comparison_text(\n",
    "            kataliz_last.explainability.semantic_fingerprint,\n",
    "            sceptyk_last.explainability.semantic_fingerprint,\n",
    "            label1=\"Katalizator\",\n",
    "            label2=\"Sceptyk\",\n",
    "            top_k=10\n",
    "        )\n",
    "        print(comparison)\n",
    "        \n",
    "        # Similarity score\n",
    "        similarity = kataliz_last.explainability.semantic_fingerprint.compare(\n",
    "            sceptyk_last.explainability.semantic_fingerprint\n",
    "        )\n",
    "        print(f\"\\nüìä Cognitive Similarity: {similarity:.3f}\")\n",
    "        print(f\"   Interpretation: {'Similar thinking' if similarity > 0.7 else 'Divergent thinking' if similarity < 0.3 else 'Moderately different'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e602c8-1058-4c49-b487-2f4f6e9e97a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Kom√≥rka 9: Save results\n",
    "output_dir = Path(\"output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save JSON\n",
    "output_file = output_dir / \"hegemon_jupyter_output.json\"\n",
    "\n",
    "output_data = {\n",
    "    \"mission\": final_state[\"mission\"],\n",
    "    \"total_cycles\": final_state[\"cycle_count\"],\n",
    "    \"final_consensus_score\": final_state.get(\"current_consensus_score\", 0.0),\n",
    "    \"contributions\": [c.model_dump() for c in final_state[\"contributions\"]],\n",
    "    \"final_plan\": final_state[\"final_plan\"].model_dump() if final_state[\"final_plan\"] else None,\n",
    "}\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(output_data, f, indent=2, default=str)\n",
    "\n",
    "print(f\"‚úÖ Output saved to: {output_file}\")\n",
    "\n",
    "# Save heatmaps\n",
    "for contrib in final_state[\"contributions\"]:\n",
    "    if contrib.explainability and contrib.explainability.semantic_fingerprint:\n",
    "        vector = contrib.explainability.semantic_fingerprint\n",
    "        heatmap = visualizer.generate_text_heatmap(vector, top_k=20)\n",
    "        \n",
    "        heatmap_file = output_dir / f\"heatmap_{contrib.agent_id}_cycle{contrib.cycle}.txt\"\n",
    "        with open(heatmap_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(heatmap)\n",
    "        \n",
    "        print(f\"‚úÖ Heatmap saved: {heatmap_file.name}\")\n",
    "\n",
    "print(\"\\nüéâ All results saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b13d3b-cb05-42a0-8bea-b503118df69e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "# KOM√ìRKA: Export Layer 2 (Epistemic Uncertainty) Data\n",
    "# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "\n",
    "from hegemon.explainability.visualizer import (\n",
    "    export_all_epistemic_profiles,\n",
    "    create_epistemic_comparison_report,\n",
    ")\n",
    "\n",
    "# 1. Export individual profiles (JSON + Text)\n",
    "exported_files = export_all_epistemic_profiles(\n",
    "    contributions=final_state[\"contributions\"],\n",
    "    output_dir=\"epistemic_exports\",\n",
    "    format=\"both\",  # \"json\", \"text\", or \"both\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"EPISTEMIC PROFILES EXPORTED\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"JSON files: {len(exported_files['json'])}\")\n",
    "print(f\"Text files: {len(exported_files['text'])}\")\n",
    "print()\n",
    "\n",
    "# Show example\n",
    "if exported_files['text']:\n",
    "    print(f\"Example text file: {exported_files['text'][0]}\")\n",
    "    with open(exported_files['text'][0], 'r') as f:\n",
    "        print(f.read()[:500])\n",
    "        print(\"...\")\n",
    "\n",
    "print()\n",
    "\n",
    "# 2. Create comparison report (Markdown)\n",
    "report_path = create_epistemic_comparison_report(\n",
    "    contributions=final_state[\"contributions\"],\n",
    "    output_path=\"epistemic_comparison.md\",\n",
    ")\n",
    "\n",
    "print(f\"Comparison report: {report_path}\")\n",
    "print()\n",
    "\n",
    "# 3. Show summary\n",
    "print(\"=\" * 70)\n",
    "print(\"LAYER 2 SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for contrib in final_state[\"contributions\"]:\n",
    "    if contrib.explainability and contrib.explainability.epistemic_profile:\n",
    "        profile = contrib.explainability.epistemic_profile\n",
    "        stats = profile.get_summary_stats()\n",
    "        \n",
    "        print(f\"{contrib.agent_id} (Cycle {contrib.cycle}):\")\n",
    "        print(f\"  Claims: {stats['total_claims']}\")\n",
    "        print(f\"  Avg Confidence: {stats['aggregate_confidence']:.2f}\")\n",
    "        print(f\"  High/Med/Low: {stats['high_confidence_count']}/\"\n",
    "              f\"{stats['medium_confidence_count']}/{stats['low_confidence_count']}\")\n",
    "        \n",
    "        # Show lowest confidence claim\n",
    "        low_claims = profile.get_low_confidence_claims(threshold=0.5)\n",
    "        if low_claims:\n",
    "            lowest = min(low_claims, key=lambda c: c.confidence)\n",
    "            print(f\"  ‚ö†Ô∏è Lowest: [{lowest.confidence:.2f}] {lowest.claim_text[:80]}...\")\n",
    "        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b24a3f-cbb3-4a86-a825-4064957b51cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Kom√≥rka 10: Display final plan\n",
    "if final_state[\"final_plan\"]:\n",
    "    plan = final_state[\"final_plan\"]\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"üìã FINAL STRATEGIC PLAN\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    print(\"## Mission Overview\")\n",
    "    print(plan.mission_overview)\n",
    "    print()\n",
    "    \n",
    "    print(f\"## Required Agents ({len(plan.required_agents)})\")\n",
    "    for i, agent in enumerate(plan.required_agents, 1):\n",
    "        print(f\"\\n{i}. {agent.role}\")\n",
    "        print(f\"   {agent.description}\")\n",
    "        print(f\"   Skills: {', '.join(agent.required_skills[:3])}...\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"## Workflow ({len(plan.workflow)} steps)\")\n",
    "    for step in plan.workflow[:5]:  # Show first 5\n",
    "        deps = f\" (depends on: {step.dependencies})\" if step.dependencies else \"\"\n",
    "        print(f\"{step.step_id}. {step.description}{deps}\")\n",
    "    if len(plan.workflow) > 5:\n",
    "        print(f\"   ... and {len(plan.workflow) - 5} more steps\")\n",
    "    print()\n",
    "    \n",
    "    print(\"## Risk Analysis\")\n",
    "    print(plan.risk_analysis)\n",
    "    print()\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"‚ùå No final plan generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5df4d0-4475-46e3-8c50-9835f9a88a8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Kom√≥rka 1: Interactive config\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "# Create toggles\n",
    "enable_expl = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Enable Explainability',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "enable_fingerprint = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Semantic Fingerprint',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=['gemini-2.0-flash-exp', 'gemini-1.5-pro', 'gpt-4o-mini'],\n",
    "    value='gemini-2.0-flash-exp',\n",
    "    description='Classifier Model:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "def apply_config(btn):\n",
    "    os.environ[\"HEGEMON_EXPLAINABILITY_ENABLED\"] = str(enable_expl.value).lower()\n",
    "    os.environ[\"HEGEMON_EXPLAINABILITY_SEMANTIC_FINGERPRINT\"] = str(enable_fingerprint.value).lower()\n",
    "    os.environ[\"HEGEMON_EXPLAINABILITY_CLASSIFIER_MODEL\"] = model_dropdown.value\n",
    "    print(\"‚úÖ Configuration applied!\")\n",
    "\n",
    "apply_btn = widgets.Button(description=\"Apply Config\", button_style='success')\n",
    "apply_btn.on_click(apply_config)\n",
    "\n",
    "# Display\n",
    "display(enable_expl, enable_fingerprint, model_dropdown, apply_btn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eaa50a-0860-48f1-a677-834eb7b4653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W Jupyter - restart kernel i test\n",
    "from hegemon.explainability.collector import ExplainabilityCollector\n",
    "\n",
    "# Sprawd≈∫ czy ma wszystkie metody\n",
    "collector_methods = [m for m in dir(ExplainabilityCollector) if not m.startswith('_')]\n",
    "print(\"Public methods:\", collector_methods)\n",
    "# Powinno pokazaƒá: ['collect', 'get_stats']\n",
    "\n",
    "private_methods = [m for m in dir(ExplainabilityCollector) if m.startswith('_collect')]\n",
    "print(\"Private collection methods:\", private_methods)\n",
    "# Powinno pokazaƒá: ['_collect_epistemic_profile', '_collect_semantic_fingerprint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de38866-de07-47b2-989c-07ce92931afd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "hegemon_env",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3.11 (hegemon_dev)",
   "language": "python",
   "name": "hegemon_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
